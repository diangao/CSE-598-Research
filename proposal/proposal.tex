\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true]{hyperref}
\usepackage{url}
\usepackage[margin=0.9in]{geometry}
\usepackage{lmodern}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{parskip}
\usepackage{enumitem}
\setlist{nosep}
\titlespacing{\section}{0pt}{1.5ex}{0.5ex}
\setlength{\parskip}{0.5em}

% Debug package for layout visualization
%\usepackage{showframe}

% Configure section headers
\titleformat{\section}{\normalsize\bfseries\scshape}{\thesection}{0.5em}{}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titlespacing{\subsection}{0pt}{1.5ex}{0.5ex}

% Add header/footer
\pagestyle{fancy}
\fancyhf{}
\rhead{CSE 598 Research Proposal}
\lhead{\thepage}

\title{CSE 598 Research Proposal: State Representation Learning for Long-Term Multi-Agent Interactions}
\author{\{diangao, anishcha, linglong\}@umich.edu \\ University of Michigan, Ann Arbor}
\date{\today}

\begin{document}
\maketitle

% Debugging note: Remember to update abstract last
\begin{abstract}
\noindent This research provides a systematic comparison of state representation architectures in multi-agent systems, analyzing GraphDB's relational modeling, VectorDB's continuous embeddings, and Semantic Memory's hybrid approach. Through controlled experiments in Tic-Tac-Toe variants and their latent-space extensions, we establish a framework for evaluating: (1) planning depth through $\tau$-bench metrics, (2) cross-domain adaptability via representation similarity analysis, and (3) memory optimization through parameter-efficient fine-tuning. Our methodology combines theoretical analysis of representation spaces with empirical validation, offering practical guidelines for architecture selection in real-time coordination systems and adaptive AI applications.
\end{abstract}

\section{Introduction and Motivation}
The effectiveness of state representation learning constitutes a fundamental challenge in developing robust multi-agent systems, particularly for applications requiring long-term interaction and cross-environment adaptability. While current LLM-based agents demonstrate proficiency in isolated decision-making tasks, three critical gaps persist:

\begin{itemize}
    \item \textbf{Representation-Environment Mismatch}: Fixed memory architectures struggle to adapt between discrete symbolic reasoning (e.g., game rules) and continuous latent-space decision-making
    
    \item \textbf{Multi-Horizon Coordination}: Existing systems lack mechanisms for maintaining coherent state representations across varying interaction timescales in multi-agent scenarios
    
    \item \textbf{Post-Training Instability}: Learned representations often degrade when deployed in environments differing from their training regimes
\end{itemize}

Our work addresses these challenges through a structured investigation of three state representation paradigms in Cognitive Language Agents:

\begin{itemize}
    \item \textbf{GraphDB}: Explicit relational modeling for strategic game trees
    \item \textbf{VectorDB}: Continuous embedding spaces for probabilistic reasoning
    \item \textbf{Semantic Memory}: Hybrid neuro-symbolic representations via LLM abstraction
\end{itemize}

Using Tic-Tac-Toe variants as our experimental testbed, we establish a controlled environment to analyze:

\begin{itemize}
    \item Discrete-to-continuous representation transitions through latent space projections
    \item Multi-agent coordination dynamics under constrained memory budgets
    \item Post-training optimization via parameter-efficient fine-tuning
\end{itemize}

This research delivers crucial insights for:
\begin{itemize}
    \item \textit{Architecture Designers}: Empirical guidelines for memory system selection based on environment characteristics
    \item \textit{Agent Practitioners}: Strategies for maintaining representation consistency in real-world deployments
    \item \textit{Theoreticians}: Quantitative framework for analyzing representation learning dynamics
\end{itemize}

\section{Research Objectives}
Our investigation establishes three principal research objectives that systematically address the core challenges in state representation learning for multi-agent systems:

\begin{enumerate}
    \item \textbf{Architecture-Specific Reasoning Capacity Analysis (Experiment 1)}
    
    This objective focuses on quantifying how different memory architectures influence strategic decision-making in structured environments. Through controlled multi-agent Tic-Tac-Toe experiments, we will:
    
    \begin{itemize}
        \item Evaluate multi-step planning efficiency through $\tau$-bench metrics
        \item Compare the planning horizon supported by GraphDB's explicit game tree representations versus VectorDB's continuous embedding strategies
        \item Measure coordination efficiency differentials through win rate analysis across grid sizes (3×3 to 4×4)
        \item Evaluate the effectiveness of Semantic Memory's RAG mechanism in reducing redundant moves through move sequence entropy calculations
    \end{itemize}

    \item \textbf{Cross-Environment Representation Transfer Assessment (Experiment 2)}
    
    This objective examines the adaptability of learned state representations across decision-making regimes. Using our continuous Tic-Tac-Toe variants, we will:
    
    \begin{itemize}
        \item Develop quantitative transferability metrics comparing discrete-to-smoothed and discrete-to-latent transitions
        \item Analyze strategy consistency through KL divergence measurements between original and projected decision distributions
        \item Validate failure recovery mechanisms by introducing controlled perturbations in continuous state spaces
    \end{itemize}

    \item \textbf{Post-Training Optimization Framework Validation (Experiment 3) (Tentative)}
    
    This tentative objective evaluates enhancement strategies for learned representations. Building on Experiments 1-2, we will:
    
    \begin{itemize}
        \item Assess LoRA fine-tuning's capacity to preserve memory stability across extended interaction horizons (100+ game iterations)
        \item Quantify COCONUT-style contrastive learning's impact on cross-architecture knowledge transfer
        \item Establish adaptation speed benchmarks for novel task variations (Connect-4 rule adaptations)
    \end{itemize}
\end{enumerate}

These objectives are systematically explored through our experimental framework, incorporating both discrete and continuous decision-making scenarios in Tic-Tac-Toe variants.

\section{Methodology}
\subsection{Data Collection}
% Debug: Specify data sources and preprocessing steps
[Describe data sources] will be collected using [tools/methods]. Preprocessing will include [steps].

\subsection{Experiments}
There are two primary experiments this paper is targeting to investigate.
\subsubsection{Discrete Task}
The goal of the discrete task experiment is to determine what form of memory storage is most ideal for performing well on a discrete task. In other words, what form of representation of the state space is best for the task performance.

The discrete state space is quite trivial: represent a given state via some matrix or vector. For example, TicTacToe can be represented via a matrix filled with 1s, $-1$s, and 0s, representing Xs, Os, and blanks, respectively.

However, the continuous state space is slightly more convoluted. The way this paper will implement this space is via a variational autoencoder. The autoencoder will be trained on representing a state in a lower dimensional continuous space which creates a latent representation of the state space. This latent representation should be continuous and will create an alternative method of comprehending the task state.

Finally, the semantic state representation is quite simple. Develop a word embedding for every possible state in the discrete task, and use that as memory.

With these state representation formats, it will be tested which form is most optimal for this discrete task. As for the task itself, this paper will implement and investigate TicTacToe.

\subsubsection{Continuous Task}
Similarly, this experiment uses the same forms of representation (i.e. discrete, continuous, semantic), however, now on a new continuous task. This task is something that the authors are still determining what is the best to follow, however, is boiled down to two potential options.

The first option is to develop some form of TicTacToe as a continuous task (i.e. introduce probabilities into the game). This would be ideal as it keeps the task relatively similar to the discrete task, and allows the authors to extract more accurately whether the type of memory storage impacts the performance on the respective types of tasks.

However, if it isn't possible to construct some form of continuous TicTacToe, then a potential backup alternative is the benchmark test HotpotQA. This benchmark tests the ability for a model to perform information recall. This may not be the same as the the previous discrete task but it still allows the authors to extrapolate information about the hypothesis.

\section{Experimental Design}
\subsection{Benchmarking}
Inspired by $\tau$-bench \& ReAct:
\begin{itemize}
    \item $\tau$-style Task Success Rate: Win percentage across multiple games
    \item State Retrieval Latency: How fast can the agent retrieve relevant past moves?
    \item Move Efficiency: Reduction in redundant moves over time
    \item Generalization Score: Can the agent adapt to 4×4 tic-tac-toe or connect-4?
\end{itemize}

\section{Expected Outcomes}
% Debug: Connect outcomes to objectives
\begin{enumerate}
    \item Discrete Memory Performs better on both Continuous and Discrete Tasks
    \subitem If this is the case, then it must be the case that the agent prefers having a discrete understanding of its current state. Moreso, the agent prefers to have steps laid out for it when it is conducting its task.
    \item Continuous Memory Performs better on both Continuous and Discrete Tasks
    \subitem If this is the case, then it must be that the agent prefers to have freedom in its decision space. In other words, the agent requires the freedom to decide its final action after reasoning.
    \item Respective Memory Type Performs better on its Respective Task
    \subitem If this is the case, then its likely the memory model should reflect the type of task being performed. In this case, researchers should evaluate the task type and implement the respective memory.
\end{enumerate}

\section{Timeline}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Timeline} & \textbf{Task} \\ 
\midrule
Weeks 1-2 & Literature review and problem formulation \\
Weeks 3-4 & Data collection framework setup \\
Weeks 5-8 & Model development and validation \\
Weeks 9-10 & Experimental analysis \\
Weeks 11-12 & Paper drafting and final submission \\
\bottomrule
\end{tabular}

\section{Code Availability}
The implementation code and experimental setup are available at: \url{https://github.com/diangao/CSE-598-Research}

\section{References}

\begin{thebibliography}{9}

\bibitem{react2022}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., \& Yang, Y. (2022).
\textit{ReAct: Synergizing Reasoning and Acting in Language Models}.
arXiv preprint arXiv:2210.03629.

\bibitem{tau2024}
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Wu, Y., ... \& Yang, Y. (2024).
\textit{$\tau$-bench: Measuring LLM-Agent Reasoning with Stateful Benchmarks}.
arXiv preprint arXiv:2406.12045.

\bibitem{coala2023}
Sumers, T. R., Joshi, A., Paras, M., Zhu, J. J., Zeng, E., Zhang, C., \& Steinhardt, J. (2023).
\textit{Cognitive Architectures for Language Agents}.
arXiv preprint arXiv:2309.02427.

\bibitem{coconut2023}
Li, X., Wu, T., Yao, S., Wang, X., \& Yang, Y. (2023).
\textit{COCONUT: Contrastive Chain-of-Thought Training for Language Model Reasoning}.
arXiv preprint arXiv:2310.18344.

\bibitem{lora2021}
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... \& Chen, W. (2021).
\textit{LoRA: Low-Rank Adaptation of Large Language Models}.
arXiv preprint arXiv:2106.09685.

\end{thebibliography}

% Add to document preamble after hyperref
\hypersetup{
    linkcolor=blue,
    citecolor=red,
    urlcolor=magenta
}

\end{document} 